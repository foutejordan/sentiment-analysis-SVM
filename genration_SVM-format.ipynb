{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e897b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da2473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction des data\n",
    "def get_data (url):\n",
    "    data = pd.read_csv(url,sep=\"\\t\", header=None)\n",
    "    data.columns = [\"id\", \"label\", \"text\"]\n",
    "    return data\n",
    "\n",
    "train_data = get_data('twitter-2013train-A.txt')\n",
    "dev_data = get_data('twitter-2013dev-A.txt')\n",
    "test_data = get_data('twitter-2013test-A.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97558014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Mise en place du lexique du corpus\n",
    "#nous supprimons toutes les ponctuations\n",
    "# avec string.punctuation: !\"#$%&'()*+, -./:;?@[\\]^_`{|}~\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#nous supprimons les espaces en trop dans un tweet\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "#tokenisation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenization(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "#Amelioration\n",
    "#Supprimons la casse\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Stemming: Ne garder que la racine des mots\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem_words(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokens]\n",
    "    return stems\n",
    "\n",
    "#Suppression des mots outils, des mots qui n'apporte pas d'information donc on peut les retirer sans affecter le sens de la phrase\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'));\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "#construction du lexique\n",
    "\n",
    "def create_lexique(data):\n",
    "    lexique = []\n",
    "    for tweet in data['text']:\n",
    "        lower_tweet = text_lowercase(tweet)\n",
    "        tweet_without_punctuation = remove_punctuation(lower_tweet)\n",
    "        new_tweet = remove_whitespace(lower_tweet)\n",
    "        tokens = tokenization(new_tweet)\n",
    "        tokens_with_stemming = stem_words(tokens)\n",
    "        tokens_wthout_stpWords = remove_stopwords(tokens_with_stemming)\n",
    "        for word in tokens:\n",
    "            if word not in lexique:\n",
    "                lexique.append(word)\n",
    "    return lexique\n",
    "    \n",
    "train_data_lexique = create_lexique(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa1177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb338499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27486"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assignation d'un numero unique a chaque lexique\n",
    "\n",
    "def assign_id_to_lexique(lexique):\n",
    "    new_lexique = {}\n",
    "    for i in range(1,len(lexique)+1):\n",
    "        new_lexique[lexique[i-1]] = i\n",
    "    return new_lexique\n",
    "\n",
    "train_lexique_with_id = assign_id_to_lexique(train_data_lexique)\n",
    "len(train_lexique_with_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119e6b7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#4) Decomptage pour chaque message, le nombre d'occurence des mots\n",
    "# [0 for i in range(len(train_data['text'])]\n",
    "\n",
    "\n",
    "def words_occurence(data, data_lexique_with_id):\n",
    "    tab = []\n",
    "    for tweet in data['text']:\n",
    "        vector = [0 for i in range(len(data_lexique_with_id)+1)]\n",
    "        for word in tokenization(tweet):\n",
    "            if word in data_lexique_with_id.keys():\n",
    "                vector[data_lexique_with_id[word]] +=1\n",
    "               \n",
    "        tab.append(vector)\n",
    "    return np.array(tab)\n",
    "\n",
    "train_tab = words_occurence(train_data, train_lexique_with_id)\n",
    "dev_tab = words_occurence(dev_data, train_lexique_with_id)\n",
    "test_tab = words_occurence(test_data, train_lexique_with_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a8d9657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3547, 27487)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896a4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) construction du fichier au format supporte par le SVM\n",
    "\n",
    "def generate_svm_file(data, tab, file):\n",
    "    dict = {}\n",
    "    label_vector = []\n",
    "    for label in data['label']:\n",
    "        if label == 'positive':\n",
    "            label = 1\n",
    "        elif label == 'negative':\n",
    "            label = 2\n",
    "        elif label == 'neutral':\n",
    "            label = 3\n",
    "        label_vector.append(label)\n",
    "    #train_svm = open(\"train.svm\", \"a\")\n",
    "    #for tweet in train_data['text']:\n",
    "    train_data\n",
    "    label_vector = np.array(label_vector)\n",
    "    #print(np.array(label_vector))\n",
    "    # il faut construire une structure qui contient, pour chauque ligne,le label, le mot avec sa clef et l'occurence\n",
    "\n",
    "    # la j'ajoute a chaque ligne le label correspondant au tweet\n",
    "    new_vector = np.insert(tab, 0, label_vector, axis=1)\n",
    "    #print(new_vector)\n",
    "    fichier = open(file, \"a\")\n",
    "    for tweet in new_vector:\n",
    "        label = str(tweet[0])\n",
    "        s = label\n",
    "        for i in range(0, len(tweet)-1):\n",
    "            c=i+1\n",
    "            var = str(tweet[c])\n",
    "            if tweet[c] != 0:\n",
    "                s = s + ' ' + str(c) + ':' + var\n",
    "        s += '\\n'\n",
    "        fichier.write(s)\n",
    "    fichier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe20ab4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "generate_svm_file(train_data, train_tab, \"train_svm.svm\")\n",
    "generate_svm_file(dev_data, dev_tab, \"dev_svm.svm\")\n",
    "generate_svm_file(test_data, test_tab, \"test_svm.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy : 59.4587%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e5449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd965426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
